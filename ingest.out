Downloading (…)e9125/.gitattributes:   0%|          | 0.00/1.18k [00:00<?, ?B/s]Downloading (…)e9125/.gitattributes: 100%|██████████| 1.18k/1.18k [00:00<00:00, 2.61MB/s]
Downloading (…)_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]Downloading (…)_Pooling/config.json: 100%|██████████| 190/190 [00:00<00:00, 469kB/s]
Downloading (…)7e55de9125/README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]Downloading (…)7e55de9125/README.md: 100%|██████████| 10.6k/10.6k [00:00<00:00, 17.6MB/s]
Downloading (…)55de9125/config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]Downloading (…)55de9125/config.json: 100%|██████████| 612/612 [00:00<00:00, 1.38MB/s]
Downloading (…)ce_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]Downloading (…)ce_transformers.json: 100%|██████████| 116/116 [00:00<00:00, 313kB/s]
Downloading (…)125/data_config.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]Downloading (…)125/data_config.json: 100%|██████████| 39.3k/39.3k [00:00<00:00, 70.0MB/s]
Downloading pytorch_model.bin:   0%|          | 0.00/90.9M [00:00<?, ?B/s]Downloading pytorch_model.bin:  12%|█▏        | 10.5M/90.9M [00:01<00:08, 9.19MB/s]Downloading pytorch_model.bin:  23%|██▎       | 21.0M/90.9M [00:01<00:05, 13.3MB/s]Downloading pytorch_model.bin:  35%|███▍      | 31.5M/90.9M [00:02<00:03, 16.9MB/s]Downloading pytorch_model.bin:  46%|████▌     | 41.9M/90.9M [00:02<00:02, 19.5MB/s]Downloading pytorch_model.bin:  58%|█████▊    | 52.4M/90.9M [00:02<00:01, 20.8MB/s]Downloading pytorch_model.bin:  69%|██████▉   | 62.9M/90.9M [00:03<00:01, 20.7MB/s]Downloading pytorch_model.bin:  81%|████████  | 73.4M/90.9M [00:03<00:00, 21.8MB/s]Downloading pytorch_model.bin:  92%|█████████▏| 83.9M/90.9M [00:04<00:00, 22.8MB/s]Downloading pytorch_model.bin: 100%|██████████| 90.9M/90.9M [00:04<00:00, 23.1MB/s]Downloading pytorch_model.bin: 100%|██████████| 90.9M/90.9M [00:04<00:00, 19.7MB/s]
Downloading (…)nce_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]Downloading (…)nce_bert_config.json: 100%|██████████| 53.0/53.0 [00:00<00:00, 344kB/s]
Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]Downloading (…)cial_tokens_map.json: 100%|██████████| 112/112 [00:00<00:00, 784kB/s]
Downloading (…)e9125/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]Downloading (…)e9125/tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 2.06MB/s]Downloading (…)e9125/tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 2.05MB/s]
Downloading (…)okenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]Downloading (…)okenizer_config.json: 100%|██████████| 350/350 [00:00<00:00, 822kB/s]
Downloading (…)9125/train_script.py:   0%|          | 0.00/13.2k [00:00<?, ?B/s]Downloading (…)9125/train_script.py: 100%|██████████| 13.2k/13.2k [00:00<00:00, 26.2MB/s]
Downloading (…)7e55de9125/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]Downloading (…)7e55de9125/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 2.07MB/s]Downloading (…)7e55de9125/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 2.05MB/s]
Downloading (…)5de9125/modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]Downloading (…)5de9125/modules.json: 100%|██████████| 349/349 [00:00<00:00, 989kB/s]
Creating new vectorstore
Loading documents from source_documents
Loading new documents:   0%|                              | 0/1 [00:00<?, ?it/s]Loading new documents: 100%|█████████████████████| 1/1 [00:00<00:00, 246.30it/s]
Using embedded DuckDB with persistence: data will be stored in: db
Loaded 1 new documents from source_documents
Split into 67 chunks of text (max. 500 tokens each)
Creating embeddings. May take some minutes...
Ingestion complete! You can now run privateGPT.py to query your documents
